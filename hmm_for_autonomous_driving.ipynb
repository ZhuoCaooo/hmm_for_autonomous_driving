{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 93\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"left_lane\", \"right_lane\"]\n",
    "observations = [\"low_speed\", \"high_speed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_model = hmm.MultinomialHMM(n_components=2,\n",
    "                                    algorithm='viterbi',  # Decoder algorithm.\n",
    "                                    # algorithm='map'  # todo: what does MaP?\n",
    "                                    random_state=seed,\n",
    "                                    n_iter=10,\n",
    "                                    tol=0.01  # EM Convergence threshold (gain in log-likelihood)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - How to easily estimate the parameters of our HMM?\n",
    "Here are the results of the estimation procedure based on **counting from the labelled data**. It constitutes the **MLE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_model.startprob_ = np.array(\n",
    "    [1/3, 2/3]\n",
    ")\n",
    "discrete_model.transmat_ = np.array([\n",
    "    [0.6, 0.4],  # P(state_t+1|state_t=state_0)\n",
    "    [0.2, 0.8]]  # P(state_t+1|state_t=state_1)\n",
    ")\n",
    "discrete_model.emissionprob_ = np.array(\n",
    "    [[0.4, 0.6],  # P(obs|state_0)\n",
    "     [0.8, 0.2]]  # P(obs|state_1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Generate random samples from the HMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 66134, 0: 33866})\n",
      "Counter({0: 66520, 1: 33480})\n"
     ]
    }
   ],
   "source": [
    "# use the parameters to generates samples\n",
    "# using a large sample size to estimate the stationary state distributions: 1/3 vs 2/3\n",
    "# (n_samples, n_features)\n",
    "Obss, States = discrete_model.sample(100000)\n",
    "States = States.flatten()\n",
    "Obss = Obss.flatten()\n",
    "from collections import Counter\n",
    "print(Counter(States))\n",
    "print(Counter(Obss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary distribution from the transition matrix\n",
    "The **stationary distribution** is a **left eigenvector** (as opposed to the usual right eigenvectors) of the **transition matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70710678 -0.4472136 ]\n",
      " [ 0.70710678 -0.89442719]]\n",
      "[0.4 1. ]\n",
      "[-0.4472136  -0.89442719]\n",
      "[0.33333333 0.66666667]\n",
      "[-0.28284271  0.28284271]\n",
      "3.5355339368654755\n",
      "[-0.4472136  -0.89442719]\n"
     ]
    }
   ],
   "source": [
    "# Compute the stationary distribution of states.\n",
    "eigvals, eigvecs = np.linalg.eig(discrete_model.transmat_.T)\n",
    "eigvec = np.real_if_close(eigvecs[:, np.argmax(eigvals)])\n",
    "print(eigvecs)\n",
    "print(eigvals)\n",
    "print(eigvec)\n",
    "# normalisation\n",
    "stat_distr = eigvec / eigvec.sum()\n",
    "print(stat_distr)\n",
    "\n",
    "# The stationary distribution is proportional to the left-eigenvector\n",
    "# associated with the largest eigenvalue (i.e., 1) of the transition matrix\n",
    "x1 = np.asarray([-0.70710678, 0.70710678 ])\n",
    "res1 = np.dot(discrete_model.transmat_.T, x1)\n",
    "print(res1)\n",
    "print(1/0.28284271)\n",
    "\n",
    "x2 = np.asarray([-0.4472136, -0.89442719])  # invariant\n",
    "res2 = np.dot(discrete_model.transmat_.T, x2)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6324555394589767"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4472136 /0.70710678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: Compute a alpha table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Given a single speed observation, what is the probability for the car to be in each of the two lanes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [readme](README.md), the **posterior conditional probabilities** were computed using Bayes' rule. For instance:\n",
    "- p(`left_lane`|`low_speed`) = p(`low_speed`|`left_lane`) * p(`left_speed`) / p(`low_speed`) \n",
    "\n",
    "**Posterior probabilities** for each state can be computed with `predict_proba()` and corresponding most likely states with `predict()`:\n",
    "- P(`left lane`  | `high speed`) = `1/3` `*` `0.6` / `1/3` = `0.6`\n",
    "- P(`left lane`  | `low speed`)  = `1/3` `*` `0.4` / `2/3` = `0.2`\n",
    "- P(`right lane` | `high speed`) = `2/3` `*` `0.2` / `1/3` = `0.4`\n",
    "- P(`right lane` | `low speed`)  = `2/3` `*` `0.8` / `2/3` = `0.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(state|'low_speed') = [[0.2 0.8]] => most likely state is 'right_lane'\n",
      "p(state|'high_speed') = [[0.6 0.4]] => most likely state is 'left_lane'\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(observations)):\n",
    "    p = discrete_model.predict_proba(np.array([[i]]))\n",
    "    most_likely = discrete_model.predict(np.array([[i]]))\n",
    "    print(\"p(state|'{}') = {} => most likely state is '{}'\".format(\n",
    "        observations[i], p, states[int(most_likely[0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach is to compute the **join probabilities**, as used in the **Viterbi algorithm**:\n",
    "- p(`left_lane`,`low_speed`) = p(`low_speed`|`left_lane`) * p(`left_speed`)\n",
    "- This is **alpha(`right_lane`, `t=1`)** for observation [`low_speed`]\n",
    "\n",
    "Join probabilities:\n",
    "- P(`left lane`  , `high speed`) = `1/3` `*` `0.6` = `3/15`  # *highest alpha for* `obs` = [`high_speed`] \n",
    "- P(`right lane` , `high speed`) = `2/3` `*` `0.2` = `2/15`\n",
    "- P(`left lane`  , `low speed`)  = `1/3` `*` `0.4` = `2/15`\n",
    "- P(`right lane` , `low speed`)  = `2/3` `*` `0.8` = `8/15`  # *highest alpha for* `obs` = [`low_speed`] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_lane -> low_speed\n",
      "prob = 0.5333333333333333\n",
      "log_prob = -0.6286086594223741\n",
      "---\n",
      "left_lane -> high_speed\n",
      "prob = 0.19999999999999998\n",
      "log_prob = -1.6094379124341005\n"
     ]
    }
   ],
   "source": [
    "# The Viterbi decoding algorithm uses the `alpha*` values. And the first `alpha*` are `alpha` values.\n",
    "# Hence, the decoding method should return the state with the **highest joint probability**.\n",
    "obs_0 = np.array([[0]]).T\n",
    "obs_1 = np.array([[1]]).T\n",
    "# Log probability of the maximum likelihood path through the HMM\n",
    "logprob_0, state_0 = discrete_model.decode(obs_0)  # 8/15 = alpha[`low_speed`](`right_lane`)\n",
    "logprob_1, state_1 = discrete_model.decode(obs_1)  # 2/5 = alpha[`high_speed`](`left_lane`)\n",
    "\n",
    "print(\"{} -> {}\".format(states[int(state_0)], observations[int(obs_0)]))\n",
    "print(\"prob = {}\\nlog_prob = {}\".format(np.exp(logprob_0), logprob_0))\n",
    "print(\"---\")\n",
    "print(\"{} -> {}\".format(states[int(state_1)], observations[int(obs_1)]))\n",
    "print(\"prob = {}\\nlog_prob = {}\".format(np.exp(logprob_1), logprob_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - What is the probability to observe a particular sequence of speed measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **marginal probabilities of an observation sequence** can be found by:\n",
    "- **The sum over any column of product `alpha` \\* `beta`**.\n",
    "- In particular, it is the **sum over the first column of the `alpha` values**:\n",
    "- In `hmmlearn`, this is given by the `score()` method.\n",
    "\n",
    "#### For a single observation\n",
    "- P(`low speed`) = P(`left lane`, `low speed`) + P(`right lane`, `low speed`) = `2/15` + `8/15` = `2/3`\n",
    "- P(`high speed`) = P(`left lane`, `high speed`) + P(`right lane`, `high speed`) = `3/15` + `2/15` = `1/3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(low_speed) = 0.6666666666666667 \n",
      "p(high_speed) = 0.33333333333333337 \n",
      "---\n",
      "p(low_speed) = 0.6666666666666667\n",
      "   posterior p(state|low_speed) = [[0.2 0.8]]\n",
      "p(high_speed) = 0.33333333333333337\n",
      "   posterior p(state|high_speed) = [[0.6 0.4]]\n"
     ]
    }
   ],
   "source": [
    "# For a single observation\n",
    "# Compute the log probability under the model.\n",
    "for i in range(len(observations)):\n",
    "    p = np.exp(discrete_model.score(np.array([[i]])))\n",
    "    print(\"p({}) = {} \".format(observations[i], p))\n",
    "\n",
    "print(\"---\")\n",
    "# Compute the log probability under the model and compute posteriors.\n",
    "for i in range(len(observations)):\n",
    "    p = (discrete_model.score_samples(np.array([[i]])))\n",
    "    print(\"p({}) = {}\\n   posterior p(state|{}) = {}\".format(observations[i], np.exp(p[0]), observations[i],  p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(['low_speed', 'high_speed', 'low_speed']) = 0.13184 \n"
     ]
    }
   ],
   "source": [
    "# For the example used: [`low_speed`, `high_speed`, `low_speed`]\n",
    "obs_sequence = np.array([[0, 1, 0]]).T\n",
    "p = np.exp(discrete_model.score(obs_sequence))\n",
    "print(\"p({}) = {} \".format([observations[i] for i in obs_sequence.T[0]], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 - Given a sequence of speed observations, what is the most likely current lane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering can be obtained by normalizing `alpha` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 - Given a sequence of speed observations, what is the most likely underlying lane sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "The **inferred optimal hidden states** can be obtained by calling `decode()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_lane -> low_speed\n",
      "right_lane -> high_speed\n",
      "right_lane -> low_speed\n",
      "\n",
      "prob = 0.05461333333333335\n",
      "log_prob = -2.9074772257991035\n"
     ]
    }
   ],
   "source": [
    "obs_sequence = np.array([[0, 1, 0]]).T\n",
    "# Find most likely state sequence corresponding to obs_sequence.\n",
    "logprob, state_sequence = discrete_model.decode(obs_sequence)\n",
    "\n",
    "# Log probability of the produced state sequence\n",
    "for o, s in zip(obs_sequence.T[0], state_sequence):\n",
    "    print(\"{} -> {}\".format(states[int(s)], observations[int(o)]))\n",
    "print(\"\\nprob = {}\\nlog_prob = {}\".format(np.exp(logprob), logprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 - How to estimate the parameters of our HMM when no state annotations are present in the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "**The HMM parameters** are estimated based on some observation data using the `fit()` method. It implements the **Baum-Welch algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random unbalanced sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `fit()` method is not \"const\". Save here the original parameters for comparison.\n",
    "old_transmat = discrete_model.transmat_\n",
    "old_emissionprob = discrete_model.emissionprob_\n",
    "old_startprob = discrete_model.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased_sampling = \n",
      "[1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0]\n",
      "---\n",
      "old_transmat = \n",
      "[[0.54907509 0.45092491]\n",
      " [0.55092127 0.44907873]]\n",
      "transmat = \n",
      "[[0.48366555 0.51633445]\n",
      " [0.48380939 0.51619061]]\n",
      "---\n",
      "old_emissionprob = \n",
      "[[0.07984999 0.92015001]\n",
      " [0.12465517 0.87534483]]\n",
      "emissionprob = \n",
      "[[0.13336734 0.86663266]\n",
      " [0.10748579 0.89251421]]\n",
      "---\n",
      "old_startprob = \n",
      "[0.58684749 0.41315251]\n",
      "startprob = \n",
      "[0.46159954 0.53840046]\n"
     ]
    }
   ],
   "source": [
    "# On purpose, one could generate an unbalanced sampling.\n",
    "biased_sampling = np.random.choice([0, 1], size=(100,), p=[1./10, 9./10])\n",
    "print(\"biased_sampling = \\n{}\".format(biased_sampling))\n",
    "\n",
    "obs_sequence = np.array([biased_sampling]).T\n",
    "new_model = discrete_model.fit(obs_sequence)\n",
    "\n",
    "# Due to random sampling, two consecutive observations are independent. Hence, an uniform transition matrix.\n",
    "# The unbalance is captured by the emission model.\n",
    "print(\"---\\nold_transmat = \\n{}\".format(old_transmat))\n",
    "print(\"transmat = \\n{}\".format(new_model.transmat_))\n",
    "print(\"---\\nold_emissionprob = \\n{}\".format(old_emissionprob))\n",
    "print(\"emissionprob = \\n{}\".format(new_model.emissionprob_))\n",
    "print(\"---\\nold_startprob = \\n{}\".format(old_startprob))\n",
    "print(\"startprob = \\n{}\".format(new_model.startprob_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo: test\n",
    "After training, it would be interesting to **assess the new model** by **submitting an observation drawn from the same distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo: monitor_\n",
    "Monitor object used to check the convergence of EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_converged_by_logprob():\n",
    "    m = ConvergenceMonitor(tol=1e-3, n_iter=10, verbose=False)\n",
    "    for logprob in [-0.03, -0.02, -0.01]:\n",
    "        m.report(logprob)\n",
    "        assert not m.converged\n",
    "\n",
    "    m.report(-0.0101)\n",
    "    assert m.converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GaussianHMM' object has no attribute 'monitor_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-0063f3b23253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 self.history[-1] >= self.tol)\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m model.monitor_ = ThresholdMonitor(model.monitor_.tol,\n\u001b[0m\u001b[0;32m     10\u001b[0m                                   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                   model.monitor_.verbose)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GaussianHMM' object has no attribute 'monitor_'"
     ]
    }
   ],
   "source": [
    "from hmmlearn.base import ConvergenceMonitor\n",
    "from hmmlearn import hmm\n",
    "class ThresholdMonitor(ConvergenceMonitor):\n",
    "    @property\n",
    "    def converged(self):\n",
    "        return (self.iter == self.n_iter or\n",
    "                self.history[-1] >= self.tol)\n",
    "model = hmm.GaussianHMM(n_components=2, tol=5, verbose=True)\n",
    "model.monitor_ = ThresholdMonitor(model.monitor_.tol,\n",
    "                                  model.monitor_.n_iter,\n",
    "                                  model.monitor_.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draft",
   "language": "python",
   "name": "draft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
